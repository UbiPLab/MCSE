DE Load balancing; Service Computing; Logistic Regression; Probabilistic
   Estimation; Machine Learning
AB Cloud computing offers scalable services to the user where computing resources are owned by a cloud provider. The resources are offered to clients on pay-per-use basis. However, since multiple clients share the cloud's resources, they could potentially interfere with each others' task during peak load instances. The environment changes every instant of time with a new set of job requests demanding resource while another set of jobs relieving another set of resources. A major challenge among the service providers is to maintain a balance without compromising Service Level Agreement (SLA). In case of peak load, when each client strives for a particular resource in minimal time, the resource allocation problem becomes more challenging. The important issue is to fulfil the SLA criterion without delaying the resource allocation.
   The paper proposes a n-player game-based Machine learning strategy that would forecast outcome using a priori information available and measure/estimate existing parameters such as utilization and delay in an optimal load-balanced paradigm. The simulation validates the conclusion of the theorem by showing that average delay is low and stays in that range as the number of job requests increase. In future, we shall extend this work to multi-resource, multi-user environment.