DE Covariance matrix estimation; Manifold learning; Multiscale analysis;
   Multiscale singular values; Intrinsic dimension estimation; Multiscale
   analysis; Geometric measure theory; Point cloud data
ID NONLINEAR DIMENSIONALITY REDUCTION; INTRINSIC DIMENSION; MANIFOLDS;
   PARAMETRIZATIONS; EIGENFUNCTIONS; APPROXIMATION; EIGENVALUES; TOPOLOGY
AB Large data sets are often modeled as being noisy samples from probability distributions mu in R-D, with D large. It has been noticed that oftentimes the support M of these probability distributions seems to be well-approximated by low-dimensional sets, perhaps even by manifolds. We shall consider sets that are locally well-approximated by k-dimensional planes, with k << D, with k-dimensional manifolds isometrically embedded in R-D being a special case. Samples from mu are furthermore corrupted by D-dimensional noise. Certain tools from multiscale geometric measure theory and harmonic analysis seem well-suited to be adapted to the study of samples from such probability distributions, in order to yield quantitative geometric information about them. In this paper we introduce and study multiscale covariance matrices, i.e. covariances corresponding to the distribution restricted to a ball of radius r, with a fixed center and varying r, and under rather general geometric assumptions we study how their empirical, noisy counterparts behave. We prove that in the range of scales where these covariance matrices are most informative, the empirical, noisy covariances are close to their expected, noiseless counterparts. In fact, this is true as soon as the number of samples in the balls where the covariance matrices are computed is linear in the intrinsic dimension of M. As an application, we present an algorithm for estimating the intrinsic dimension of M. (C) 2015 Elsevier Inc. All rights reserved.