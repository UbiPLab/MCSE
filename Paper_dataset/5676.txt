DE Cloud computing; Task scheduling; Energy efficiency; Queueing model;
   Q-learning
AB High energy consumption has become a growing concern in the operation of complex cloud data centers due to the ever-expanding size of cloud computing facilities and the ever-increasing number of users. It is critical to find viable solutions to cloud task scheduling so that cloud resources can be utilized in an energy-efficient way while still meeting diverse user requirements in real time. In this research we propose a Q-learning based task scheduling framework for energy-efficient cloud computing (QEEC). QEEC has two phases. In the first phase a centralized task dispatcher is used to implement the M/M/S queueing model, by which the arriving user requests are assigned to each server in a cloud. In the second phase a Q-learning based scheduler on each server first prioritizes all the requests by task laxity and task life time, then uses a continuously-updating policy to assign tasks to virtual machines, applying incentives to reward the assignments that can minimize task response time and maximize each server's CPU utilization. We have conducted simulation experiments, which have confirmed that implementing a M/M/S queueing system in a cloud can help to reduce the average task response time. The experiments have also demonstrated that the QEEC approach is the most energy-efficient as compared to other task scheduling policies, which can be largely credited to the M/M/S queueing model and the Q-learning strategy implemented in QEEC. (C) 2020 Elsevier B.V. All rights reserved.