DE Edge computing; Container; Resource management; Auto-scalability
AB Conveying the workload of IoT systems from the cloud to edge nodes have been widely adopted by industrial and academic sectors. This tendency is generally promoted to meet the requirements of some time-sensitive use cases such as IoT healthcare applications. However, IoT devices at the edge network are likely to be resource-limited, as well as, they perform under an extremely heterogeneous environment in terms of the connected devices and the deployed software modules. Thus, both of the aforementioned concerns have considerably led to hindering the deployment process of services on IoT edge devices. In this paper, we propose an approach to facilitate a scalable and lightweight solution for service deployment for efficient resource utilization on IoT edge nodes. Our solution is based on the container concept, and we adopt the cluster concept to define a group of IoT edge devices. Containers are lightweight virtualization technique that enables services to be packaged and deployed with their dependencies regardless of the hosts infrastructure, as well as, they facilitate the service communication and the update process. Furthermore, containers are supported by some means of orchestration such as swarm. These orchestration tools can be configured to enable services deployment and resources sharing among IoT edge devices falling within the same cluster. However, they lack elasticity in terms of auto-scaling up/down of services instances in corresponding to the resource utilization of all cluster elements, as well as, service performance metrics. Our approach overcomes these limitations by following an auto-scaling process based on MAPE-K loop, which is based on our proposed rule model to generate a scaling plan by analyzing collected performance metrics of a cluster. Our evaluation shows the efficiency of the proposed approach in adapting the system performance to meet service performance requirements and the availability of system resources.