DE Peer-to-peer computing; Task analysis; Channel allocation; Cloud
   computing; Servers; Data centers; Switches; Congestion control; data
   delivery; edge cloud; prioritized bandwidth allocation
AB As the key infrastructure for emerging 5G and Internet-of-Things (IoT) applications, micro data centers would be widely deployed at network edges to provide high-bandwidth low-latency cloud service. In these systems, applications would deliver large-size data objects among servers for various purposes like service deployment, application scale-up, and data duplication on demand. Accordingly, reducing delivery time is crucial for the optimization of service delay and system utilization. To accelerate the delivery, this article proposes a multisource-aware adaptive data transmission solution, Parallel Push (PPUSH), by leveraging the fact that data objects in the cloud are generally replicated among servers by design. At the high level, PPUSH achieves efficient delivery of multisource data by launching multiple push flows in parallel; and at the low level, it decouples transfers from different sources by encoding data objects with rateless RaptorQ code, and further employing novel congestion controls to prioritize the bandwidth allocation of concurrent tasks respecting their remaining sizes. Fluid model analysis along with Mininet-based test and packet-level simulation shows that, unlike DCTCP and other proposals, push is robust to packet loss and achieves provable prioritized bandwidth allocation. Extensive simulation results imply that, with above advantages, PPUSH could achieve very efficient data delivery by making use of all available data sources: for instance, compared with the straightforward design of equal-size task split and fair bandwidth allocation, its adaptive task assignment and prioritized traffic scheduling reduce the average task completion time in a tested scenario by 1.495x and 1.329x, respectively, demonstrating a total improvement of 1.586x, when enabled at the same time.