DE Web crawler; Cloud computing; Architecture; Service; Web page; Virtual
   machine
AB The web search is a rich and wide topic of research. Web crawlers are the key and initiative step in search engines. The web crawler is responsible for collecting web pages to be indexed. Web crawling's techniques, architecture, scalability and implementation are factors that influence crawler performance. Web crawler architectures are competitive to cover a high percent of World Wide Web and should be designed on a large scale and in a distributed architecture. The cloud computing offers elastic features to alleviate this challenge; it is highly needed that crawlers can be built in a scalable fashion. As the creation of such scaling crawlers is very complex, this paper proposes a concept of Crawler as a Service similar to those web services which are available in any region with collaborative distributed fashion over World Wide Web and APIs to offload scaling to another layer of abstraction. Crawler stages are designed as separate services and loosely coupled components. Each service can be configured and provided as separate and scalable over different remote areas. This crawler architecture can be customized to crawl a specific field, region, or language. This proposed architecture is based on adaptive customization and standardization for cloud services, so this architecture is more usable and fueled for numerous communities and cloud customers in creating own search engine.